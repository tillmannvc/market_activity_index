{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### IMPORTS #####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import BytesIO\n",
    "from google.cloud import storage\n",
    "client = storage.Client()\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from MAI2023.masterFunctions_v20240502 import *\n",
    "import geopandas as gpdcheckRunningOrders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infoVars(df, mktID, locGroup): # assign info variables based on date and location\n",
    "    df['mktID'] = mktID\n",
    "    df['locGroup'] = locGroup\n",
    "    country = checkLocationFileStatus(mktID, 'country')\n",
    "    df['country'] = country\n",
    "    display(df['ident'])\n",
    "    try: # Necessary because some exports have band names starting with 1_ or 2_, not the date. Comes from merge of two image collections ic_old and ic_new\n",
    "        df['date'] = pd.to_datetime(df['ident'].apply(lambda x: datetime.strptime(x[:8], \"%Y%m%d\").date()))\n",
    "    except:\n",
    "        df['date'] = pd.to_datetime(df['ident'].apply(lambda x: datetime.strptime(x[2:10], \"%Y%m%d\").date()))\n",
    "    try:\n",
    "        df['time'] = df['ident'].apply(lambda x: datetime.strptime(x[9:15], \"%H%M%S\").time())\n",
    "    except:\n",
    "        df['time'] = df['ident'].apply(lambda x: datetime.strptime(x[11:17], \"%H%M%S\").time())\n",
    "        \n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    #df['time_decimal']=   df['time'].dt.hour + df['time'].dt.minute / 60 + df['time'].dt.second / 3600\n",
    "    df['time_decimal'] = df['time'].apply(lambda t: t.hour + t.minute / 60 + t.second / 3600)\n",
    "    df['weekday'] = (df['date'].dt.weekday + 1) % 7\n",
    "    df['mkt_lat'] = pd.to_numeric(df['mktID'].str.extract(r'lon(-?\\d+)_(\\d+)').apply(lambda x: f\"{x[0]}.{x[1]}\", axis=1))\n",
    "    df['mkt_lon'] = pd.to_numeric(df['mktID'].str.extract(r'lat(-?\\d+)_(\\d+)').apply(lambda x: f\"{x[0]}.{x[1]}\", axis=1))\n",
    "    if country==\"Kenya\": # For some locations in Kenya, the lon and lat coordinates were flipped in their mktid\n",
    "        df['origLat'] = df['mkt_lat']\n",
    "        df.loc[df['mkt_lat'] > 30, 'mkt_lat'] = df['mkt_lon']\n",
    "        df.loc[df['mkt_lon'] < 30, 'mkt_lon'] = df['origLat']\n",
    "        df.drop(columns=['origLat'], inplace=True)\n",
    "    if country==\"Ethiopia\": # For some locations in Ethiopia, the lon and lat coordinates were flipped in their mktid\n",
    "        df['origLat'] = df['mkt_lat']\n",
    "        df.loc[df['mkt_lat'] > 20, 'mkt_lat'] = df['mkt_lon']\n",
    "        df.loc[df['mkt_lon'] < 20, 'mkt_lon'] = df['origLat']\n",
    "        df.drop(columns=['origLat'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def identifyMktDays(df,minRank, threshold_for_market): # identify market days based on detected areas and their threshold values\n",
    "    print('minRank: ', minRank, ', Threshold_for_market: ', threshold_for_market)\n",
    "    # List all maximum threshold values on the days-of-week where we detected something and that detection falls below a threshold \n",
    "    min_thres_by_day = df[df['strictnessRank'] <= threshold_for_market].groupby('weekdayThisAreaIsActive')['strictnessRank'].min()\n",
    "    if loc in invalid_market_days: # remove  known misdetections from visual checks of outputs\n",
    "        value_to_remove = invalid_market_days[loc]\n",
    "        if value_to_remove in min_thres_by_day:\n",
    "            del min_thres_by_day[value_to_remove]\n",
    "    print('strictness rank and active weekdays',min_thres_by_day)\n",
    "    # Find the clearest detection \n",
    "    lowest_thres = min_thres_by_day.min()\n",
    "    print('lowest strictness rank',lowest_thres)\n",
    "    # Filter unique days of week where the threshold is within 3 ranks of the lowest threshold value -> identifies all similarly high detections\n",
    "    localMktDays = list(min_thres_by_day[min_thres_by_day - lowest_thres <= 3].index.unique())\n",
    "    print('localMktDays', localMktDays)\n",
    "    def find_position(weekday):\n",
    "        try:\n",
    "            return list(localMktDays).index(weekday)\n",
    "        except ValueError:\n",
    "            return -1  # Return 0 if the weekday is not found in the list\n",
    "    df['pos'] = df['weekday'].apply(find_position)\n",
    "    df['mktDay'] = None\n",
    "    df.loc[(df['weekday'] == df['weekdayThisAreaIsActive']) & (df['pos'] >= 0), 'mktDay'] = 1 # detected market day\n",
    "    df.loc[(df['weekday'] != df['weekdayThisAreaIsActive']) & (df['pos'] == -1), 'mktDay'] = 0 # detected non-market day\n",
    "    df.loc[(df['weekday'] != df['weekdayThisAreaIsActive']) & (df['pos'] >= 0), 'mktDay'] = 99 # observation of detected market area for a given weekday on a different weekday\n",
    "    return df\n",
    "\n",
    "def cleanActMeasures(df, geos, varsOfInterest): \n",
    "    # Set values to NA that exceed the median value per market, weekday of operation\n",
    "    # and instrument by more than twice the IQR , calculated over the period \n",
    "    # outside Covid and for typical times and good images\n",
    "    #df['time_decimal'] = df['ident'].apply(extract_time_decimal)\n",
    "    df['diff_to_median_time'] = df.apply(lambda row: abs(row['time_decimal'] - df['time_decimal'].median()), axis=1)\n",
    "    mask = (\n",
    "        (df['date'].between('2020-03-01', '2021-02-28')) | # potentially covid-affected\n",
    "        (df['date'] < '2018-01-01') |                      # generally noisier because of sparse imagery\n",
    "        (df['diff_to_median_time'] > .5) |                  # differing sun angle\n",
    "        ((df['clear_percent'].notnull()) & (df['clear_percent'] < 10)) | # noisy imagery\n",
    "        ((df['cloud_percent'].notnull()) & (df['cloud_percent'] > 50))\n",
    "    )\n",
    "    # Create a new column 'exclDates' based on the mask\n",
    "    df['exclDates'] = mask.astype(int)\n",
    "    for b in geos: # within each possible area\n",
    "        df[f'sumsum_maxpMax_{b}'] = df[f'sumsum_maxpMax_{b}'] / df[f'ccount_maxpMax_{b}'] # convert sum variable into mean deviations\n",
    "\n",
    "        # Typical number of pixels per shape\n",
    "        max_count = df.loc[df['exclDates'] != 1].groupby(['weekdayThisAreaIsActive', 'mktDay'])[f'ccount_maxpMax_{b}'].max().reset_index()\n",
    "        df = pd.merge(df, max_count, on=[ 'weekdayThisAreaIsActive', 'mktDay'], how='outer', suffixes=('', '_max_count'))        \n",
    "\n",
    "        for p in varsOfInterest:\n",
    "            try:\n",
    "                # set to NA those values coming from images that cover less than 50% of the typical footprint\n",
    "                df.loc[df[f'ccount_maxpMax_{b}']  < 0.5 *(df[f'ccount_maxpMax_{b}_max_count']), f'{p}_maxpMax_{b}'] = np.nan\n",
    "\n",
    "                # calculate median, iqr by detected area and sensor, and merge to dataframe\n",
    "                median = df.loc[df['exclDates'] != 1].groupby(['weekdayThisAreaIsActive', 'mktDay', 'instrument'])[f'{p}_maxpMax_{b}'].quantile(0.5).reset_index()\n",
    "                df = pd.merge(df, median, on=[ 'weekdayThisAreaIsActive', 'mktDay', 'instrument'], how='outer', suffixes=('', '_median'))\n",
    "\n",
    "                p25 = df.loc[df['exclDates'] != 1].groupby(['weekdayThisAreaIsActive', 'mktDay', 'instrument'])[f'{p}_maxpMax_{b}'].quantile(0.25)\n",
    "                p75 = df.loc[df['exclDates'] != 1].groupby(['weekdayThisAreaIsActive', 'mktDay', 'instrument'])[f'{p}_maxpMax_{b}'].quantile(0.75)\n",
    "                iqr = (p75-p25).reset_index()\n",
    "                df = pd.merge(df, iqr, on=[ 'weekdayThisAreaIsActive', 'mktDay', 'instrument'], how='outer', suffixes=('', '_iqr'))\n",
    "                \n",
    "                # set to NA those values that are more than twice the IQR above the median\n",
    "                df.loc[df[f'{p}_maxpMax_{b}']  > (df[f'{p}_maxpMax_{b}_median'] + 2 * df[f'{p}_maxpMax_{b}_iqr']), f'{p}_maxpMax_{b}'] = np.nan\n",
    "                df = df.drop([f'{p}_maxpMax_{b}_median', f'{p}_maxpMax_{b}_iqr'], axis=1)    \n",
    "\n",
    "            except Exception as e:\n",
    "                print('Error in cleanActMeasures', e)\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "def contains_substring(s, substrings):\n",
    "    for substring in substrings:\n",
    "        if substring in s:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def drop_columns_by_pattern(df, patterns_to_drop):\n",
    "    for pattern in patterns_to_drop:\n",
    "        try:\n",
    "            df = df.drop(df.filter(like=pattern).columns, axis=1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while dropping columns for pattern '{pattern}': {e}\")\n",
    "    return df\n",
    "\n",
    "def determine_sensor(row):\n",
    "    image_id = row['ident']\n",
    "    condition1 = '3B' in image_id[-2:]\n",
    "    condition2 = '_1_' in image_id\n",
    "    if condition1 or condition2:\n",
    "        return 'PS2'\n",
    "    else:\n",
    "        return 'PSB.SD'\n",
    "    \n",
    "def prepare_properties(locGroup, loc, propToDrop):\n",
    "    df_prop = pd.read_csv(f'gs://exports-mai2023/{locGroup}/properties/propEx_{locGroup}_{loc}.csv')\n",
    "    \n",
    "    # Extract 'ident' from 'system:index' column\n",
    "    df_prop['ident'] = df_prop['system:index'].str.slice(stop=23) \n",
    "    # Determine the imagery generation of each image\n",
    "    df_prop['instrument'] = df_prop.apply(determine_sensor, axis=1)\n",
    "    print(df_prop['instrument'].unique())\n",
    "    # Drop specified properties from the DataFrame\n",
    "    for prop in propToDrop:\n",
    "        try:\n",
    "            df_prop = df_prop.drop(prop, axis=1)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return df_prop  \n",
    "\n",
    "def identify_varying_areas(wide_df): # Identify the largest ring in which P75 non-market day activity still does not exceed P50 market day activity\n",
    "    market_days = wide_df.loc[wide_df['mktDay'] == 1, 'weekday'].unique().tolist()\n",
    "    gdfs = [] # dataframe to hold the selected shapes\n",
    "    for market_day in market_days:\n",
    "        print('market_days', market_days, market_day)\n",
    "\n",
    "        df_mktDays = wide_df[(wide_df['mktDay'] == 1) \n",
    "                     & (wide_df['exclDates'] == 0) \n",
    "                     & (wide_df['clear_percent'] > 90) \n",
    "                     & (wide_df['weekdayThisAreaIsActive']==market_day) \n",
    "                     & (wide_df['weekday']==market_day) \n",
    "                     & (wide_df['diff_to_median_time'] <.5)]\n",
    "        \n",
    "        filtered_columns_sum = df_mktDays.loc[:, df_mktDays.columns.str.contains('sumsum') & \n",
    "                                     ~df_mktDays.columns.str.contains('_100')]\n",
    "\n",
    "        # Exclude columns that are all NA\n",
    "        filtered_columns_sum = filtered_columns_sum.loc[:, filtered_columns_sum.notna().any()].columns.tolist()\n",
    "        \n",
    "        df_nonmktDays = wide_df[(wide_df['mktDay'] == 0) \n",
    "                                & (wide_df['exclDates'] == 0) \n",
    "                                & (wide_df['clear_percent'] > 90)\n",
    "                                & (wide_df['diff_to_median_time'] <.5) \n",
    "                                & (wide_df['weekdayThisAreaIsActive']==market_day)]\n",
    "        p75_nonmktDays_sum = df_nonmktDays[filtered_columns_sum].dropna(subset=filtered_columns_sum, how='all').quantile(0.75)    \n",
    "    \n",
    "        # keep high quality images, separately for market and non-market days\n",
    "\n",
    "        # Calculate variance and mean for percentiles (filtered_columns_p)\n",
    "        p50_mktDays_sum = df_mktDays[filtered_columns_sum].dropna(subset=filtered_columns_sum, how='all').quantile(0.5)\n",
    "        result = pd.concat([p50_mktDays_sum, p75_nonmktDays_sum], axis=1)\n",
    "        result.columns = ['p50_mktDays_sum', 'p75_nonmktDays_sum']\n",
    "\n",
    "        print(result)\n",
    "        first_row_index = (result['p75_nonmktDays_sum'] > result['p50_mktDays_sum']).replace(False, np.nan).idxmax()\n",
    "        if pd.isna(first_row_index):\n",
    "            first_row_index= result.iloc[-1].name\n",
    "\n",
    "        print(\"First row where p75_nonmktDays_sum > p50_mktDays_sum: \",first_row_index)\n",
    "\n",
    "        # Update DataFrame with name of area per weekday that we consider the market area\n",
    "        wide_df[f'maxVar_s_{market_day}_maxpMax'] = first_row_index\n",
    "        #print(loc,first_row_index)\n",
    "        filtered_gdf = select_areas(market_day, first_row_index)\n",
    "        gdfs.append(filtered_gdf)\n",
    "\n",
    "    return wide_df, gdfs\n",
    "\n",
    "def select_areas(market_day,first_row_index): #select the shapes associated with the selected market area\n",
    "    # extract substring between second last and last instance of _\n",
    "    temp = first_row_index.split('_')\n",
    "    if len(temp) >= 2:\n",
    "        minRing =  int(temp[-2])\n",
    "    else:\n",
    "        minRing = None  # Return None if there aren't enough parts\n",
    "    print('minRing', minRing)\n",
    "    # load shapefile    \n",
    "    shp_path = f'gs://exports-mai2023/{locGroup}/shapes/shp_MpM6_{locGroup}{loc}.shp'\n",
    "    gdf = gpd.read_file(shp_path)    \n",
    "    filtered_gdf = gdf[(gdf['weekdayShp'] == market_day) & \n",
    "                   (gdf['strictness'] == minRing) & \n",
    "                   (gdf['subStrictn'] == 100)].copy()\n",
    "    #filtered_gdf.plot()\n",
    "    filtered_gdf.loc[:, 'mktid'] = loc  # Use .loc to set values\n",
    "    return filtered_gdf\n",
    "\n",
    "def prepend_zero_if_single_digit(value):\n",
    "    if len(str(value)) == 1:\n",
    "        return '0' + str(value)\n",
    "    else:\n",
    "        return str(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify locations\n",
    "#cnx = mysql.connector.connect(user='root',password='XXX',host='XXX',database='mai-database')\n",
    "\n",
    "query = '''\n",
    "SELECT l.Location\n",
    "FROM `mai-database`.`location_file` l\n",
    "WHERE EXISTS (\n",
    "    SELECT 1\n",
    "    FROM `mai-database`.`process_runs` pr\n",
    "    WHERE pr.Location = l.Location\n",
    "    AND pr.Process = '04ActivityExport'\n",
    "    AND pr.Setup = 'exportAct5'\n",
    "    AND pr.Status = 'complete'\n",
    ")\n",
    "'''\n",
    "cursor = cnx.cursor()\n",
    "cursor.execute(query)\n",
    "response = cursor.fetchall()\n",
    "cursor.close()\n",
    "cnx.close()\n",
    "\n",
    "locs = [row[0] for row in response]\n",
    "print(locs, len(locs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(locs, len(locs))\n",
    "locs=list(set(locs))\n",
    "freqDayStr_short='w7'\n",
    "maxRank = 4 # exclude altitude levels above this\n",
    "ring_area_share = 0.8  # Only consider rings whose area is more than 100(1-X)% of the shape defining the outer border of the ring\n",
    "\n",
    "prefix = \"exports-mai2023\"\n",
    "target_folder ='activity_cleaned_2024'\n",
    "varsOfInterest=['p50','sumsum', 'ccount']\n",
    "threshold_for_market=24\n",
    "bucket=client.get_bucket('exports-mai2023')\n",
    "\n",
    "forMerge=['ident','weekdayThisAreaIsActive','date','mktDay','mktID','locGroup','time','year','month', 'weekday', 'mkt_lat','mkt_lon','time_decimal'] \n",
    "patterns_to_drop = ['ground_control','strictnessRank', 'subStrictnessRank''Geography','origName_', 'coorLength_', '.geo', 'system:index_b0', 'system:index', 'weekday_','market']\n",
    "propToDrop=['quality_category','system:index', '.geo','order_id', 'pixel_resolution','gsd','provider', 'published', 'publishing_stage', 'item_type', 'item_id', 'snow_ice_percent', 'strip_id','updated']\n",
    "print(locs, len(locs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-heater",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store DataFrames from each location\n",
    "list_df_acrossLocs = []\n",
    "pd.set_option('display.width', 100)  # Set display width\n",
    "pd.set_option('display.max_columns', 500)  # Show all columns\n",
    "    \n",
    "def check_file_exists(bucket, file_path):\n",
    "    \"\"\"Check if a file exists in a Google Cloud Storage bucket.\"\"\"\n",
    "    blob = bucket.blob(file_path)\n",
    "    return blob.exists()\n",
    "\n",
    "invalid_market_days = {\n",
    "    'lon37_6468lat0_0505': 0, # small parking lot outside football stadion\n",
    "    'lon39_1245lat-4_5529': 3 # noise \n",
    "}\n",
    "\n",
    "problemLocs=[]\n",
    "\n",
    "locCount=0\n",
    "for loc in sorted(locs, reverse=False):\n",
    "    print('loc', loc)\n",
    "    locCount=locCount+1\n",
    "    if check_file_exists(client.get_bucket('exports-mai2023'), f\"{target_folder}/df_{loc}.csv\") and check_file_exists(client.get_bucket('exports-mai2023'), f\"{target_folder}/shp_{loc}.shp\"):\n",
    "        #print(f\"The files df_{loc}.csv and df_{loc}.shp already exist.\")\n",
    "        pass\n",
    "        \n",
    "    else:    \n",
    "        try:\n",
    "            # Get the GEEbucket and locGroup for the current location\n",
    "            GEEbucket = checkLocationFileStatus(loc, 'bucket')\n",
    "            locGroup = checkLocationFileStatus(loc, 'locGroup')\n",
    "            print(loc, GEEbucket, locGroup)\n",
    "            # prepare image property dataframe to be merged in later\n",
    "            df_prop = prepare_properties(locGroup, loc, propToDrop)\n",
    "\n",
    "            # Read the activity CSV file\n",
    "            df = pd.read_csv(f'gs://exports-mai2023/{locGroup}/measures/exportAct5_maxpMax{loc}_{freqDayStr_short}.csv')\n",
    "            \n",
    "            # keep only entries that fall between the strictest rank we define and the least strict one for a given shape, but at least 30\n",
    "            minRank = max(df['strictnessRank'].min(),30)\n",
    "            df = df[(df['strictnessRank'] <= minRank) & (df['strictnessRank'] >= maxRank)]\n",
    "            df = df[((df['subStrictnessRank'] <= minRank) & (df['subStrictnessRank'] > maxRank)) | (pd.isna(df['subStrictnessRank'])) | (df['subStrictnessRank'] ==100) ]\n",
    "            print(df['strictnessRank'].unique().tolist())\n",
    "            df['subStrictnessRank'] = df['subStrictnessRank'].fillna(100).astype(int)\n",
    "\n",
    "            eligible_rings = df[df['subStrictnessRank'] != 100].groupby('strictnessRank', as_index=False)['subStrictnessRank'].max()\n",
    "            additional_rows = pd.DataFrame({\n",
    "                'strictnessRank': df['strictnessRank'].unique(),\n",
    "                'subStrictnessRank': 100\n",
    "            })\n",
    "            eligible_shapes = pd.concat([eligible_rings, additional_rows]).sort_values(by='strictnessRank').reset_index(drop=True)\n",
    "\n",
    "            df_elig =  pd.merge(df, eligible_shapes, on=['strictnessRank', 'subStrictnessRank'])\n",
    "\n",
    "            df_elig.rename(columns={'weekdayShp': 'weekdayThisAreaIsActive'}, inplace=True)\n",
    "\n",
    "            # Extract image id \n",
    "            df_elig['ident'] = df_elig['ident'].str.rsplit('_maxpMax', n=1).str[0].str[1:] \n",
    "            df_elig['weekdayThisAreaIsActive'] = df_elig['weekdayThisAreaIsActive'].astype(int)\n",
    "            df_elig['strictnessRank'] = df_elig['strictnessRank'].astype(int)\n",
    "\n",
    "            # Create area_id column from the strictnessRank variables\n",
    "            df_elig['strictnessRank_str'] = df_elig['strictnessRank'].apply(prepend_zero_if_single_digit)\n",
    "            df_elig['subStrictnessRank_str'] = df_elig['subStrictnessRank'].apply(prepend_zero_if_single_digit)\n",
    "            df_elig['area_id'] = df_elig['strictnessRank_str'].astype(str) + '_' + df_elig['subStrictnessRank_str'].astype(str)\n",
    "\n",
    "            geos = df_elig['area_id'].unique()\n",
    "\n",
    "            # Append area id to variable names\n",
    "            new_column_names = {old_col: old_col + '_maxpMax'  for old_col in varsOfInterest}\n",
    "            df_elig = df_elig.rename(columns=new_column_names)\n",
    "            \n",
    "            # Assign info variables\n",
    "            df_elig = infoVars(df_elig, loc, locGroup)\n",
    "            \n",
    "            # Identify market days\n",
    "            df_elig = identifyMktDays(df_elig,minRank, threshold_for_market)\n",
    "            \n",
    "            wide_df = df_elig.pivot_table(index=forMerge, columns='area_id', values=list(new_column_names.values()))\n",
    "            wide_df.columns = ['_'.join(str(s).strip() for s in col if s) for col in wide_df.columns]\n",
    "            wide_df.reset_index(inplace=True)    \n",
    "\n",
    "            # Drop unnecessary columns\n",
    "            wide_df = drop_columns_by_pattern(wide_df, patterns_to_drop)\n",
    "\n",
    "            # Merge with properties\n",
    "            wide_df = pd.merge(wide_df, df_prop, on='ident', how='left')\n",
    "            \n",
    "            # Exclude outliers\n",
    "            wide_df = cleanActMeasures(wide_df, geos, varsOfInterest)\n",
    "            \n",
    "            # Identify varying areas on market days\n",
    "            wide_df, market_shapes_list = identify_varying_areas(wide_df)\n",
    "            \n",
    "            # Upload activity CSV to GCS\n",
    "            blob = bucket.blob(f\"{target_folder}/df_{loc}.csv\")\n",
    "            blob.upload_from_string(wide_df.to_csv(index=False), content_type='text/csv')\n",
    "\n",
    "            # Export GeoDataFrame to ESRI Shapefile format\n",
    "            tmp_file = '/temp_shapefile.shp' \n",
    "            market_shapes = gpd.GeoDataFrame(pd.concat(market_shapes_list, ignore_index=True),\n",
    "                                             crs=market_shapes_list[0].crs)\n",
    "\n",
    "            market_shapes.to_file(tmp_file, driver='ESRI Shapefile')\n",
    "\n",
    "            # Upload each file of the shapefile to GCS\n",
    "            for ext in ['shp', 'shx', 'dbf', 'prj']:\n",
    "                blob = bucket.blob(f\"{target_folder}/shp_{loc}.{ext}\")\n",
    "                blob.upload_from_filename(f\"{tmp_file.replace('.shp', '.' + ext)}\")\n",
    "\n",
    "            print(f'{loc} exported! {locCount}/{len(locs)}')\n",
    "        except Exception as e:\n",
    "            print(f'problem with {loc}',e)\n",
    "            problemLocs.extend([loc])   \n",
    "            pass\n",
    "print(problemLocs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPORTING SHAPES\n",
    "names = [\"MOZ_20240702\",\"MWI_20240702\",\"ETH_20240702\", \"KEN_20240702\"]\n",
    "countries = [\"Mozambique\", \"Malawi\", \"Ethiopia\", \"Kenya\"]\n",
    "for name, country in zip(names, countries):\n",
    "    \n",
    "    ## Merge all csvs from an area of interest into one master file\n",
    "    dataset_name = name\n",
    "    target_folder ='activity_cleaned_20240704'\n",
    "    geo_of_interest = country\n",
    "    level = \"country\"\n",
    "\n",
    "    query = f'''\n",
    "    SELECT l.Location\n",
    "    FROM `mai-database`.`location_file` l\n",
    "    WHERE l.{level} = '{geo_of_interest}' \n",
    "    AND EXISTS (\n",
    "        SELECT 1\n",
    "        FROM `mai-database`.`process_runs` pr\n",
    "        WHERE pr.Location = l.Location\n",
    "        AND pr.Process = '04ActivityExport'\n",
    "        AND pr.Setup = 'exportAct5'\n",
    "        AND pr.Status = 'complete'\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    cursor = cnx.cursor()\n",
    "    cursor.execute(query)\n",
    "    response = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    cnx.close()\n",
    "\n",
    "    locs = [row[0] for row in response]\n",
    "    print(locs, len(locs))\n",
    "\n",
    "    list_df_acrossLocs=[]\n",
    "    loc_counter=0\n",
    "    batch_counter = 0\n",
    "\n",
    "    gdfs = []\n",
    "    for loc in locs:\n",
    "        print(loc)\n",
    "        try:\n",
    "            gdf = gpd.read_file(f\"gs://exports-mai2023/{target_folder}/shp_{loc}.shp\", driver='ESRI Shapefile')\n",
    "            gdfs.append(gdf)   \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {loc}: {e}\")\n",
    "            pass\n",
    "            \n",
    "    merged_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), crs=gdfs[0].crs)\n",
    "    # Export the merged GeoDataFrame to a new shapefile\n",
    "    merged_gdf.to_file(f'shp_{dataset_name}.shp', driver='ESRI Shapefile')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-payment",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## EXPORTING ACTIVITY MEASURES\n",
    "## Identify locations\n",
    "\n",
    "names = [\"UG\"]#, \"ETH_20240702\", \"MOZ_20240702\"]\n",
    "countries = [\"Uganda\"]#, \"Ethiopia\",\"Mozambique\"]\n",
    "for name, country in zip(names, countries):\n",
    "    #cnx = mysql.connector.connect(user='root',password='XXX',host='XXX',database='mai-database')\n",
    "\n",
    "    ## Merge all csvs from an area of interest into one master file\n",
    "    dataset_name = name\n",
    "    target_folder ='activity_cleaned_2024'\n",
    "    geo_of_interest = country\n",
    "    level = \"country\"\n",
    "\n",
    "    query = f'''\n",
    "    SELECT l.Location\n",
    "    FROM `mai-database`.`location_file` l\n",
    "    WHERE l.{level} = '{geo_of_interest}' \n",
    "    AND EXISTS (\n",
    "        SELECT 1\n",
    "        FROM `mai-database`.`process_runs` pr\n",
    "        WHERE pr.Location = l.Location\n",
    "        AND pr.Process = '04ActivityExport'\n",
    "        AND pr.Setup = 'exportAct5'\n",
    "        AND pr.Status = 'complete'\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    cursor = cnx.cursor()\n",
    "    cursor.execute(query)\n",
    "    response = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    cnx.close()\n",
    "\n",
    "    locs = [row[0] for row in response]\n",
    "    print(locs, len(locs))\n",
    "\n",
    "    list_df_acrossLocs=[]\n",
    "    loc_counter=0\n",
    "    batch_counter = 0\n",
    "\n",
    "    def replace_after_underscore(s):\n",
    "            return s[:s.rfind('_') + 1] + '100'\n",
    "\n",
    "    for loc in locs:\n",
    "        print(loc)\n",
    "        filePath = f'gs://exports-mai2023/{target_folder}/df_{loc}.csv'\n",
    "\n",
    "        # Read the CSV file\n",
    "        #try:\n",
    "        df = pd.read_csv(filePath)\n",
    "        df = df.drop(columns=df.filter(like='count').columns)        \n",
    "        #display(df)\n",
    "        loc_counter += 1\n",
    "        #print(len(list(df.columns)))\n",
    "        market_days = df.loc[df['mktDay'] == 1, 'weekday'].unique().tolist()\n",
    "        tokeep=[]\n",
    "        for market_day in market_days:\n",
    "            #print(market_day)\n",
    "            target_var = df[f\"maxVar_s_{market_day}_maxpMax\"].unique().tolist()[0].replace(\"maxpmax\", \"maxpMax\")\n",
    "            tokeep.extend([df[f\"maxVar_s_{market_day}_maxpMax\"].unique().tolist()[0].replace(\"maxpmax\", \"maxpMax\")])\n",
    "            df.loc[(market_day == df['weekdayThisAreaIsActive']) , 'activity_measure'] = df[target_var]\n",
    "\n",
    "        mean_nonmktday =  df.loc[(df['instrument'] != 'PS2') & (df['mktDay']==0)].groupby(['weekdayThisAreaIsActive', 'instrument'])['activity_measure'].mean().reset_index()\n",
    "        df = pd.merge(df, mean_nonmktday, on=[ 'weekdayThisAreaIsActive', 'instrument'], how='outer', suffixes=('', '_mean_nonmktday'))\n",
    "        df['activity_measure_mean0'] = df['activity_measure']-df['activity_measure_mean_nonmktday']\n",
    "        mean_mktday = df.loc[(df['instrument'] != 'PS2') & (df['mktDay']==1) & df['year']==2021].groupby(['weekdayThisAreaIsActive', 'instrument'])['activity_measure_mean0'].mean().reset_index()\n",
    "        df = pd.merge(df, mean_mktday, on=[ 'weekdayThisAreaIsActive', 'instrument'], how='outer', suffixes=('', '_mean_mktday'))\n",
    "        df['activity_measure_norm']=100*df['activity_measure_mean0']/df['activity_measure_mean0_mean_mktday']\n",
    "\n",
    "        # Apply the function to each string in listA\n",
    "        tokeep_100 = [replace_after_underscore(col) for col in tokeep]        \n",
    "        cols_to_drop = [col for col in df.columns if ('maxVar' not in col and 'maxpMax' in col) and col not in tokeep and col not in tokeep_100 ]\n",
    "        #print(cols_to_drop)\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        df = df.drop(columns=['ground_control', 'time', 'ident', 'locGroup'])\n",
    "\n",
    "        list_df_acrossLocs.append(df)\n",
    "\n",
    "        #print(len(list(df.columns)))\n",
    "        print('here', loc_counter)\n",
    "        #stop\n",
    "        # Export and reset for every 50 locations\n",
    "        if loc_counter % 50 == 0:\n",
    "            df_acrossLocs = pd.concat(list_df_acrossLocs, ignore_index=True)\n",
    "\n",
    "            # Rearrange column order\n",
    "            new_order = ['mktID', 'weekdayThisAreaIsActive']\n",
    "            df_acrossLocs = df_acrossLocs[new_order + [col for col in df_acrossLocs.columns if col not in new_order]]\n",
    "\n",
    "            # Save to CSV\n",
    "            df_acrossLocs.to_csv(f'df_{dataset_name}_batch{batch_counter}.csv', index=False)\n",
    "            print('saved batch', batch_counter)\n",
    "            # Reset lists and counters for the next batch\n",
    "            list_df_acrossLocs = []\n",
    "            batch_counter += 1\n",
    "\n",
    "        #except Exception as e:\n",
    "        #    print(f\"Error with {loc}: {e}\")\n",
    "        #    pass\n",
    "\n",
    "    # Handle the final batch if it has fewer than 50 locations\n",
    "    if list_df_acrossLocs:\n",
    "        df_acrossLocs = pd.concat(list_df_acrossLocs, ignore_index=True)\n",
    "\n",
    "        # Rearrange column order\n",
    "        new_order = ['mktID',  'weekdayThisAreaIsActive']\n",
    "        df_acrossLocs = df_acrossLocs[new_order + [col for col in df_acrossLocs.columns if col not in new_order]]\n",
    "\n",
    "        # Save to CSV\n",
    "        df_acrossLocs.to_csv(f'df_{dataset_name}_batch{batch_counter}.csv', index=False)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
